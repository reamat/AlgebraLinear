% Este trabalho está licenciado sob a Licença Creative Commons Atribuição-CompartilhaIgual 3.0 Não Adaptada. Para ver uma cópia desta licença, visite https://creativecommons.org/licenses/by-sa/3.0/ ou envie uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\documentclass[../livro.tex]{subfiles}  %%DM%%Escolher document class and options article, etc

%define o diretório principal
\providecommand{\dir}{..}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%% Pacotes básicos para MathEnvir%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \usepackage{amsmath}   %%AMS primary package (includes amstext, amsopn, amsbsy), provides various features for displayed equations and %%other mathematical constructs.
% %%%% OPTIONS FOR THE AMSMATH PACKAGE
% \usepackage{amscd}     %%Provides a CD environment for simple commutative diagrams (no support for diagonal arrows).
% \usepackage{amsxtra}   %%Provides certain odds and ends such as \fracwithdelims and \accentedsymbol, for compatibility with documents %%created using version 1.1.
% \usepackage{amsthm}    %%Enhanced version of \newtheorem command for defining theorem-like environments
% \usepackage{amssymb}   %%Provides an extended symbol collection (includes amsfonts). For example, \barwedge, \boxdot, \boxminus, %%\boxplus, \boxtimes, \Cap, \Cup (and many more), the arrow \leadsto, and some other symbols such as \Box and \Diamond.

% \usepackage{latexsym}  %%makes few additional characters available: \Box \Join \Box \Diamond \leadsto \sqsubset \sqsupset \lhd \unlhd %%\rhd \unrhd

% \usepackage[makeroom]{cancel}   % Cancelar termos em equações

% \usepackage{enumerate}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%% Links -- Só funciona para .pdf%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % \usepackage{hyperref}  %% for references %%Options below
% % \hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, linktoc=page, pdftitle={Shadowing}, pdfauthor={D. Marcon}}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%% Color, graphicx, margin (geometry)%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \usepackage[dvips]{graphicx}
% \usepackage{color}            %%Textcolor, color definitions, etc

% % \definecolor{light-blue}{rgb}{0.8,0.85,1}     %%Numbers between 0 and 1
% % \definecolor{mygrey}{gray}{0.75}              %%Numbers between 0 and 1

% \usepackage{verbatim}         %%Adds text from other files, comment environment

% \usepackage{xpatch}           %%Bold theorem titles
% \makeatletter
% \xpatchcmd{\@thm}{\fontseries\mddefault\upshape}{}{}{} %same font as thm-header
% \makeatother

% \usepackage[margin=1in]{geometry}  %%Margins  %%Possible to use \newgeometry to modify small parts mid-document



% \usepackage{tgbonum}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%% Para escrever português%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% % \usepackage[utf8]{inputenc}  %%encoding
% % \usepackage[T1]{fontenc}     %%encoding

% \usepackage[portuguese]{babel}   %%Portuguese-specific commands



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%% Theorem styles, numbering%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \theoremstyle{plain}
% \newtheorem{theorem}{Teorema}              %%section, chapter, etc
% \newtheorem{proposition}[theorem]{Proposição}      %%everything below obeys theorem because: [theorem]
% \newtheorem{lemma}[theorem]{Lema}
% \newtheorem{corollary}[theorem]{Corolário}
% \newtheorem{maintheorem}{Teorema}                   %%number doesn't obey order
% \renewcommand{\themaintheorem}{\Alph{maintheorem}}
% \newtheorem{maincorollary}{Corolário}
% \newtheorem{conjecture}{Conjectura}
% \newtheorem*{claim}{Afirmação}

% \newtheorem*{desloct}{Segundo Teorema de Deslocamento -- Deslocamento em $t$}     %%Sem numeração e com o nome desejado

% \theoremstyle{definition}
% \newtheorem{remark}[theorem]{Observação}
% \newtheorem{example}[theorem]{Exemplo}
% \newtheorem{definition}[theorem]{Definição}
% \newtheorem{exercise}{Exercício}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%% New Commands%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newcommand{\mb}[1]{\mathbb{#1}}
% \newcommand{\bC}{\mathbb{C}}
% \newcommand{\bE}{\mathbb{E}}
% \newcommand{\bK}{\mathbb{K}}
% \newcommand{\bN}{\mathbb{N}}
% \newcommand{\bP}{\mathbb{P}}
% \newcommand{\bQ}{\mathbb{Q}}
% \newcommand{\bR}{\mathbb{R}}
% \newcommand{\bS}{\mathbb{S}}
% \newcommand{\bT}{\mathbb{T}}
% \newcommand{\bZ}{\mathbb{Z}}

% \newcommand{\mathcal{B}}{\mathcal{B}}
% \newcommand{\cE}{\mathcal{E}}
% \newcommand{\cF}{\mathcal{F}}
% \newcommand{\cH}{\mathcal{H}}
% \newcommand{\cL}{\mathcal{L}}
% \newcommand{\cM}{\mathcal{M}}
% \newcommand{\cO}{\mathcal{O}}
% \newcommand{\cP}{\mathcal{P}}
% \newcommand{\cQ}{\mathcal{Q}}
% \newcommand{\cR}{\mathcal{R}}
% \newcommand{\cS}{\mathcal{S}}

% \newcommand{\al} {\alpha}       \newcommand{\Al}{\Alpha}
% \newcommand{\be} {\beta}        \newcommand{\Be}{\Beta}
% \newcommand{\ga} {\gamma}       \newcommand{\Ga}{\Gamma}
% \newcommand{\de} {\delta}       \newcommand{\De}{\Delta}
% \newcommand{\ep} {\epsilon}
% \newcommand{\eps}{\varepsilon}
% \newcommand{\ze} {\zeta}
% \newcommand{\vte}{\vartheta}
% \newcommand{\iot}{\iota}
% \newcommand{\ka} {\kappa}
% \newcommand{\la} {\lambda}      \newcommand{\La}{\Lambda}
% \newcommand{\vpi}{\varpi}
% % \newcommand{\ro} {\rho}
% \newcommand{\vro}{\varrho}
% \newcommand{\si} {\sigma}       \newcommand{\Si}{\Sigma}
% \newcommand{\vsi}{\varsigma}
% \newcommand{\ups}{\upsilon}     \newcommand{\Up}{\Upsilon}
% \newcommand{\vphi}{\varphi}
% \newcommand{\om} {\omega}       \newcommand{\Om}{\Omega}

% \newcommand{\ang}{\operatorname{angle}}
% \newcommand{\closu}{\operatorname{clos}}
% \newcommand{\Col}{\operatorname{Col}}
% \newcommand{\const}{\operatorname{const}}
% \newcommand{\curl}{\operatorname{curl}}
% \newcommand{\dd}{\, \mathrm{d}}
% \newcommand{\diam}{\operatorname{diam}}
% \newcommand{\Dim}{\operatorname{dim}}
% \newcommand{\Div}{\operatorname{div}}
% \newcommand{\dist}{\operatorname{dist}}
% \newcommand{\grad}{\operatorname{grad}}
% \newcommand{\fr}{\partial}
% \newcommand{\graph}{\operatorname{graph}}
% \newcommand{\id}{\operatorname{Id}}
% \newcommand{\inter}{\operatorname{int}}
% \newcommand{\Leb}{\operatorname{Leb}}
% \newcommand{\length}{\operatorname{length}}
% \newcommand{\Lip}{\operatorname{Lip}}
% \newcommand{\Nul}{\operatorname{Nul}}
% \newcommand{\proj}{\operatorname{proj}}
% \newcommand{\res}{\operatornamewithlimits{Res}}
% \newcommand{\rot}{\operatorname{rot}}
% \newcommand{\sen}{\operatorname{sen}}
% \newcommand{\senh}{\operatorname{senh}}
% \newcommand{\Span}{\operatorname{Span}}
% \newcommand{\spec}{\operatorname{spec}}
% \newcommand{\supp}{\operatorname{supp}}
% \newcommand{\var}{\operatornamewithlimits{{var}}}
% \newcommand{\intd}{\operatornamewithlimits{{\iint}}}


% \everymath{\displaystyle}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%% INFO  DO  ARTIGO %%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \title{Álgebra Linear -- Semana 13}
% \author{}
% \date{20 de Junho de 2017}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% INICIO DO DOCUMENTO%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% \maketitle
% \tableofcontents

% \vspace{0.5cm}

\chapter{Semana 13}


\section{Método dos Mínimos Quadrados}

Sistemas lineares aparecem como modelos matemáticos de vários fenômenos e em várias situações. Acontece que alguns sistemas simplesmente não possuem soluções e ficamos sem saber como proceder. O \textbf{Método dos Mínimos Quadrados} é uma técnica que nos permite, de forma aproximada, retirar alguma informação desses sistemas impossíveis. A terminologia se deve ao fato de que, como veremos, este método minimiza a soma dos quadrados dos erros obtidos na aproximação.

Como sabemos, resolver o sistema linear
\begin{equation}
A \vec{x} = \vec{b}
\end{equation} consiste em encontrar um vetor $\vec{x}$ que satisfaça a esta equação. Na terminologia que construimos ao longo do curso, isto significa que $\vec{b}$ pertence ao espaço coluna da matriz $A$, isto é, $\vec{b} \in \operatorname{Col} A$. Desta forma, não é possível resolver o sistema quando $\vec{b} \not\in \operatorname{Col} A$.
\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\linewidth]{\dir/Semana13/semana13-proj-col.png}
  \end{center}
\end{figure}

O Método dos Mínimos Quadrados consiste em olhar para o vetor $\vec{p} = \proj_{\operatorname{Col} A} \vec{b}$ e resolver o sistema linear associado
\begin{equation}
A \vec{x} = \vec{p}.
\end{equation} Esta solução de $A \vec{x} = \vec{p}$ é chamada de \textbf{solução de mínimos quadrados}. A ideia é (ver figura) considerar o vetor em $\operatorname{Col} A$ que é ``mais próximo'' de $\vec{b}$ e cujo sistema linear associado possua solução. Neste contexto, estamos pensando em mais próximo no sentido que
\begin{equation}
\| \vec{b} - \proj_{\operatorname{Col} A} \vec{b} \| \le \| \vec{b} - \vec{c} \|, \text{ para qualquer } \vec{c}\, \in \operatorname{Col} A,
\end{equation} isto é, no sentido de ser a projeção a melhor aproximação de $\vec{b}$ no espaço coluna de $A$. Escrevendo os vetores em coordenadas:
\begin{equation}
\vec{b} =
\begin{bmatrix}
  b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix} \ \text{e} \
\proj_{\operatorname{Col} A} \vec{b} =
\begin{bmatrix}
  p_1 \\ p_2 \\ \vdots \\ p_n
\end{bmatrix},
\end{equation} temos que o valor
\begin{equation}
\| \vec{b} - \proj_{\operatorname{Col} A} \vec{b} \| = \sum_{i=1}^n (b_i - p_i)^2
\end{equation} é chamado de \textbf{erro da aproximação}. Logo, como anunciado, a soma dos quadrados dos erros obtidos cada componente é o mínimo possível.


\begin{example}\label{exp:minquad1}
  Considere o sistema linear
  \begin{equation}
  \left\{
    \begin{array}{ll}
      x_1 + x_2 - 2x_3 = 3 \\
      x_1  - 2x_3 = 2 \\
      x_2 + 2x_3 = 0 \\
      x_1 + x_2 - 2x_3 = 0
    \end{array}
  \right. \ \leftrightsquigarrow  \
  A \vec{x} = \begin{bmatrix}
    1 & 1 & -2 \\ 1 & 0 & -2 \\ 0 & 1 & 2 \\ -1 & -1 & 2
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix} =
  \begin{bmatrix}
    3 \\ 2 \\ 0 \\ 0
  \end{bmatrix} = \vec{b}.
  \end{equation} Este sistema é claramente impossível, pois a primeira equação é inconsistente com a última. De forma mais automática, um passo no escalonamento da matriz aumentada associada revelaria a mesma conclusão:
  \begin{equation}
  [\, A \ | \ \vec{b} \, ] = \begin{bmatrix}
    1 & 1 & -2 & 3 \\ 1 & 0 & -2 & 2 \\ 0 & 1 & 2 & 0 \\ -1 & -1 & 2 & 0
  \end{bmatrix}   \sim
  \begin{bmatrix}
    1 & 1 & -2 & 3 \\ 1 & 0 & -2 & 2 \\ 0 & 1 & 2 & 0 \\ 0 & 0 & 0 & 3
  \end{bmatrix} .
  \end{equation} Vamos tentar encontrar, conforme descrito acima, uma solução de mínimos quadrados. Passos a serem realizados:
  \begin{itemize}
  \item Para calcular a projeção de $\vec{b}$ sobre $\operatorname{Col} A$, precisamos, em primeiro lugar, obter uma base ortogonal do espaço $\operatorname{Col} A$; isto pode ser feito pelo Processo de Gram--Schmidt;
  \item Calcular a projeção $\vec{p} = \proj_{\operatorname{Col} A} \vec{b}$;
  \item Resolver o sistema linear $A \vec{x} = \vec{p}$.
  \end{itemize} Embora seja possível realizar estes três passos, temos de convir que seria muito trabalhoso. Por isto, vamos analisar um pouco mais da teoria para encontrar um método mais eficaz$. \ \lhd$
\end{example}

Suponhamos que:
\begin{itemize}
\item $A$ é uma matriz $m \times n$,
\item $\vec{x} \in \mathbb{R}^n$,
\item $\vec{b} \in \mathbb{R}^m$ e
\item o sistema $A \vec{x} = \vec{b}$ não possui solução.
\end{itemize} A solução de mínimos quadrados é a solução de $A \vec{x} = \vec{p}$, onde $\vec{p} = \proj_{\operatorname{Col} A} \vec{b}$. Observamos, por outro lado, que, sendo $\vec{p}\,$ a projeção sobre o espaço coluna de $A$, o vetor $\vec{b} - \vec{p}\,$ deve ser ortogonal a todos os elementos de $\operatorname{Col} A$. Em particular, se escrevermos $A$ em termos de suas colunas
\begin{equation}
A =
\begin{bmatrix}
  | & | &   & | \\
  \vec{a}_1 & \vec{a}_2 & \cdots  & \vec{a}_n \\
  | & | &   & |
\end{bmatrix}, \qquad \left( \text{que,  em particular, implica }
  A^T = \begin{bmatrix}
    \text{---} & \vec{a}_1  & \text{---} \\
    \text{---} & \vec{a}_2  & \text{---} \\
    & \vdots     &  \\
    \text{---} & \vec{a}_n  & \text{---}
  \end{bmatrix}\right)
\end{equation} devemos ter
\begin{equation}
\vec{a}_j \cdot \big(\vec{b} - \vec{p}\big) = 0, \text{ para todo } j  \ \  \iff \ \
A^T (\vec{b} - \vec{p}) = \vec{0} \ \ \iff \ \ A^T \vec{b} = A^T \vec{p} = A^T A\vec{x}.
\end{equation} Esta última linha é válida porque o produto escalar $\vec{a}_j \cdot \big(\vec{b} - \vec{p}\big)$ é justamente a entrada $j$ do vetor obtido ao multiplicar $A^T(\vec{b} - \vec{p})$.

Concluimos, desta forma, que, se $\vec{x}$ é uma solução de mínimos quadrados, ou seja, se $A \vec{x} = \vec{p}$, então necessariamente
\begin{equation}\label{minquad}
  \boxed{A^T A\vec{x} = A^T \vec{b}.}
\end{equation}

\begin{exercise}[Teórico]
  Justifique que o conjunto de soluções do sistema \eqref{minquad} coincide com o conjunto das soluções de mínimos quadrados do sistema $A \vec{x} = \vec{b}$.
\end{exercise}

Tudo isto implica que podemos utilizar a equação \eqref{minquad} para encontrar as soluções de mínimos quadrados de $A \vec{x} = \vec{b}$.

\begin{example}[De volta ao Exemplo \ref{exp:minquad1}]\label{exp:minquad2}
  Calculamos
  \begin{equation}
  A^T A =
  \begin{bmatrix}
    1 & 1 & 0 & -1 \\ 1 & 0 & 1 & -1 \\ -2 & -2 & 2 & 2
  \end{bmatrix}
  \begin{bmatrix}
    1 & 1 & -2 \\ 1 & 0 & -2 \\ 0 & 1 & 2 \\ -1 & -1 & 2
  \end{bmatrix} =
  \begin{bmatrix}
    3 & 2 & -6 \\ 2 & 3 & -2 \\ -6 & -2 & 16
  \end{bmatrix}
  \end{equation} e
  \begin{equation}
  A^T \vec{b} =
  \begin{bmatrix}
    1 & 1 & 0 & -1 \\ 1 & 0 & 1 & -1 \\ -2 & -2 & 2 & 2
  \end{bmatrix}
  \begin{bmatrix}
    3 \\ 2 \\ 0 \\ 0
  \end{bmatrix} =
  \begin{bmatrix}
    5 \\ 3 \\ -10
  \end{bmatrix}
  \end{equation} Para resolvermos o sistema $A^TA\vec{x} = A^T\vec{b}$, vamos escalonar a matriz aumentada associada:
  \begin{equation}
  \begin{bmatrix}
    3 & 2 & -6 & 5 \\
    2 & 3 & -2 & 3 \\
    -6 & -2 & 16 & -10
  \end{bmatrix} \sim
  \begin{bmatrix}
    3 & 2 & -6 & 5 \\
    0 & 5 &  6 & -1 \\
    0 & 2 &  4 & 0
  \end{bmatrix} \sim
  \begin{bmatrix}
    3 & 2 & -6 & 5 \\
    0 & 5 &  6 & -1 \\
    0 & 0 &  4 & 1
  \end{bmatrix}
  \end{equation} Logo,
  \begin{equation}
  \left\{
    \begin{array}{ll}
      3 x_1 + 2 x_2 - 6 x_3 = 5 \\
      5x_2 + 6 x_3 = -1 \\
      4x_3 = 1
    \end{array}
  \right..
  \end{equation} Assim,
  \begin{equation}
  x_3 = \frac{1}{4} \implies 5x_2 + 6 \cdot \frac{1}{4} = -1 \implies x_2 = -\frac{1}{2} \implies 3 x_1 - 1 - \frac{3}{2}  = 5 \implies x_1 = \frac{3}{2}.
  \end{equation} Portanto, uma solução de mínimos quadrados é
  \begin{equation}
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix} =
  \begin{bmatrix}
    3/2 \\ -1/2 \\ 1/4
  \end{bmatrix}.
  \end{equation} Observe como isto é muito menos trabalhoso do que o método que havíamos esquematizado quando discutimos este sistema no Exemplo \ref{exp:minquad1}$. \ \lhd$
\end{example}

\begin{exercise}
  Colocar em prática o método discutido no Exemplo \ref{exp:minquad1} e comparar o resultado com o que obtivemos no Exemplo \ref{exp:minquad2}.
\end{exercise}

\begin{remark}
  Caso a matriz $A$ já seja uma matriz ortogonal, podemos calcular projeções diretamente, pois uma base ortogonal para $\operatorname{Col} A$ já esté disponível desde o começo. Neste caso, ambos os métodos exigiriam um trabalho parecido.
\end{remark}

Nossa aproximação linear foi encontrada de modo a minimizar o erro quadrático. Para encontrar o erro, deveríamos calcular
\begin{equation}
\| \vec{b} - \proj_{\vec{\operatorname{Col} A}} \vec{b} \|.
\end{equation} No entanto, resolvemos de outra maneira e não calculamos a projeção. Observando que a solução de mínimos quadrados satisfaz $A \vec{x} = \proj_{\vec{\operatorname{Col} A}} \vec{b}$, podemos calcular o erro por
\begin{equation}
\text{erro } = \| \vec{b} - A \vec{x} \|,
\end{equation} onde $\vec{x}$ é a solução de mínimos quadrados.

No Exemplo \ref{exp:minquad2} acima, podemos calcular o erro desta forma. De fato,
\begin{equation}
A \vec{x} =
\begin{bmatrix}
  1 & 1 & -2 \\
  1 & 0 & -2 \\
  0 & 1 &  2 \\
  -1 & -1&  2
\end{bmatrix}
\begin{bmatrix}
  3/2 \\ -1/2 \\ 1/4
\end{bmatrix} =
\begin{bmatrix}
  1/2 \\ 1 \\ 0 \\ -1/2
\end{bmatrix} \implies
\vec{b} - A\vec{x} =
\begin{bmatrix}
  5/2 \\ 1 \\ 0 \\ 1/2
\end{bmatrix}
\end{equation}
\begin{equation}
\implies \text{erro } = \| \vec{b} - A \vec{x} \| = \sqrt{\frac{25}{4} + 1 + \frac{1}{4}} = \frac{\sqrt{30}}{2}.
\end{equation}

\subsection*{Exercícios resolvidos}

\construirExeresol

\subsection*{Exercícios}

\construirExer

\section{Fatoração QR e Mínimos Quadrados}

Vimos que o processo de Gram-Schmidt nos permite escrever uma matriz $A$ cujas linhas são linearmente independentes na forma $A=QR$, onde Q é uma matriz ortogonal, $R$ é triangular superior invertível, e as colunas de $Q$ formam uma base ortonormal para o espaço-coluna de $A$. Chamamos esta fatoração de $A$ de fatoração QR.

A fatoração QR tem uma importante aplicação na solução de problemas em mínimos quadrados. Ocorre que, em muitos problemas, a matriz $A^T A$ é uma matriz {\it mal-condicionada}, ou seja uma matriz que é muito sensível aos erros de aproximação cometidos em cálculos numéricos usualmente executados no tratamento de problemas aplicados.
Os exemplos que virão a seguir nos mostram situações em que há uma grande variação de magnitude entre as entradas da matriz $A^T A$, e isto costuma ser fonte de instabilidade numérica.

Para isso, a fatoração $QR$ oferece uma solução alternativa em mínimos quadrados para um sistema linear $Ax=b$:
Se $A=QR$, então
\begin{equation}
A^T=R^T Q^T\end{equation}
% \;\; \mbox{e}
e
\begin{equation}  A^T A= R^T Q^T Q R= R^T R,
\end{equation}
onde usamos o fato de que $Q^{-1}=Q^T$.
Assim as equações normais $A^T A \vec{x} = A^T \vec{b}$ se reduzem a
\begin{equation} R^T R \vec{x} = R^T Q^T \vec{b}.\end{equation}
Ainda, como $R$ é invertível podemos eliminar (cancelar) $R^T$ na equação acima, %ficando com a seguinte expressão para a solução em mínimos quadrados
obtendo o seguinte:
\begin{theorem}
  Se $A=QR$ com $Q$ ortogonal e $R$ triangular superior invertível, então
  a solução em mínimos quadrados do sistema $Ax=b$ é única e dada pela solução do sistema
  \begin{equation}  R \vec{x} =  Q^T \vec{b}.\end{equation}
\end{theorem}

Note que o fato de $R$ ser triangular superior torna a resolução do sistema acima pouco custosa do ponto de vista computacional.

Alternativamente temos uma expressão explícita para solução em mínimos quadrados: \begin{equation}   \vec{x} = R^{-1} Q^T \vec{b}.\end{equation}

É importante notar que existem algoritmos para fatoração QR que são \it{numéricamente estáveis}, ou seja pouco suscetiveis a erros de aproximação ou arredondamento. Tais algorimos são diferentes do processo de Gram Schmidt usual.

\subsection*{Exercícios resolvidos}

\construirExeresol

\subsection*{Exercícios}

\construirExer

\section{Regressão Linear Simples}

Vamos apresentar uma aplicação de algumas das técnicas da seção anterior à Estatística ou Econometria. Uma \textbf{regressão linear simples} é uma equação, tipicamente da forma
\begin{equation}
y = a + b x,
\end{equation} para estimar os valores $(x,y)$ apenas conhecendo alguns valores específicos $(x_1, y_1), (x_2, y_2), \dots,$ $(x_k, y_k)$. A ideia é tentar capturar como que mudanças na variável independente $x$ afetam a variável dependente $y$.

\begin{example}\footnote{Exemplo adaptado de \url{https://onlinecourses.science.psu.edu/stat501/node/257}}\label{exp:idade}
  Com o intuito de analisar se é razoável supor que há um relação linear entre a idade de um motorista e quão longe ele consegue ver, uma empresa (Last Resource, Inc., Bellefonte, PA) coletou dados de 30 motoristas. Para simplificar as nossas contas, vamos listar abaixo \textit{apenas alguns} destes dados.
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
      Idade & Distância (em $m$) \\ \hline
      20 & 590 \\
      32 & 410 \\
      41 & 460 \\
      49 & 380 \\
      66 & 350 \\
      \hline
    \end{tabular}
  \end{center} Podemos pensar em $y$ como a distância e em $x$ como a idade. Gostaríamos de achar uma relacção linear da forma
  \begin{equation}
  y = a + b x.
  \end{equation} Desta forma, os dados obtidos implicam que
  \begin{equation}
  \left\{
    \begin{array}{ll}
      b + 20 a = 590 \\
      b + 32 a = 410 \\
      b + 41 a = 460 \\
      b + 49 a = 380 \\
      b + 66 a = 350
    \end{array}
  \right. \ \ \leftrightsquigarrow \ \
  \begin{bmatrix}
    1 & 20 \\
    1 & 32 \\
    1 & 41 \\
    1 & 49 \\
    1 & 66 \\
  \end{bmatrix}
  \begin{bmatrix}
    a \\ b
  \end{bmatrix} =
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix}
  \end{equation} Ora, dificilmente um sistema linear com duas incógnitas e cinco equações terá solução (só terá solução se todos os pontos do conjunto de dados estiverem alinhados em uma reta perfeita!). Vamos procurar por uma solução de mínimos quadrados. Isto é \textbf{regressão linear simpes}.

  Denotando
  \begin{equation}
  A =
  \begin{bmatrix}
    1 & 20 \\
    1 & 32 \\
    1 & 41 \\
    1 & 49 \\
    1 & 66 \\
  \end{bmatrix} \quad \text{e} \quad
  \vec{b} =
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix}
  \end{equation} precisamos calcular
  \begin{equation}
  A^T A =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 \\
    20 & 32 & 41 & 49 & 66
  \end{bmatrix}
  \begin{bmatrix}
    1 & 20 \\
    1 & 32 \\
    1 & 41 \\
    1 & 49 \\
    1 & 66 \\
  \end{bmatrix} =
  \begin{bmatrix}
    5    & 208 \\
    208  & 9832  \\
  \end{bmatrix}
  \end{equation} e
  \begin{equation}
  A^T \vec{b} =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 \\
    20 & 32 & 41 & 49 & 66
  \end{bmatrix}
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix} =
  \begin{bmatrix}
    2190 \\ 85500
  \end{bmatrix}.
  \end{equation} Em seguida, a matriz associada aumentada pode ser reduzida por escalonamento:
  \begin{equation}
  [\, A^TA \ | \ \vec{b} \, ] =
  \begin{bmatrix}
    5    & 208 & 2190 \\
    208  & 9832 & 85500 \\
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 468510/737 \\
    0 & 1 & -7005/1474 \\
  \end{bmatrix} \implies
  \left\{
    \begin{array}{ll}
      a \simeq 635.7 \\
      b \simeq -4.75
    \end{array}
  \right..
  \end{equation} Os números são feios, mas as contas feitas foram as que sempre fizemos ao escalonar uma matriz até sua forma escalonada reduzida.

  A conclusão é que a reta de mínimos quadrados que melhor aproxima os nossos dados é a reta
  \begin{equation}
  y = a + b x = 635.7 - 4.75 x.
  \end{equation} O erro de mínimos quadrados nesta aproximação (ou norma do resíduo) pode ser calculado como
  \begin{equation}
  A \vec{x} = \begin{bmatrix}
    1 & 20 \\
    1 & 32 \\
    1 & 41 \\
    1 & 49 \\
    1 & 66 \\
  \end{bmatrix}
  \begin{bmatrix}
    635.7 \\
    4.75 \\
  \end{bmatrix} \simeq
  \begin{bmatrix}
    730.7 \\
    787.7 \\
    830.45 \\
    868.45 \\
    949.2 \\
  \end{bmatrix} \simeq \proj_{\operatorname{Col} A} \vec{b} \implies \text{ erro } = \| \vec{b} - A\vec{x} \| \simeq 947.3
  \end{equation} Na figura abaixo, mostramos um esboço da reta que melhor aproxima os dados deste exemplo$. \ \lhd$
  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=1\linewidth]{\dir/Semana13/semana13-idade.png}
    \end{center}
  \end{figure}
\end{example}



De uma maneira geral, para encontrar uma \textbf{reta de melhor ajuste} a uma quantidade de pontos (dados coletados de algum problema)
\begin{equation}
(x_1, y_1), \ (x_2, y_2), \ \dots, (x_k, y_k),
\end{equation} devemos procurar pela solução $(a,b)$ de mínimos quadrados do sistema linear
\begin{equation}
\left\{
  \begin{array}{rl}
    a + b x_1 &\!\!\!\!\!= y_1  \\
    a + b x_2 &\!\!\!\!\!= y_2  \\
    \vdots &  \\
    a + b x_k &\!\!\!\!\!= y_k  \\
  \end{array}
\right. \quad \leftrightsquigarrow  \quad
\begin{bmatrix}
  1 & x_1 \\
  1 & x_2 \\
  \vdots & \vdots \\
  1 & x_k \\
\end{bmatrix}
\begin{bmatrix}
  a \\ b
\end{bmatrix} =
\begin{bmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_k
\end{bmatrix}.
\end{equation} Vejamos um outro exemplo:

\begin{example}
  Encontrar a reta que melhor se ajusta aos pontos do plano
  \begin{equation}
  (1,2), \ \ (2,3), \ \ (2,4), \ \ (3,2), \ \ (4,3), \ \ (4,4), \ \ (5,3), \ \ (5,5), \ \ (6,4).
  \end{equation} Como vimos acima, podemos considerar
  \begin{equation}
  A =
  \begin{bmatrix}
    1 & 1 \\
    1 & 2 \\
    1 & 2 \\
    1 & 3 \\
    1 & 4 \\
    1 & 4 \\
    1 & 5 \\
    1 & 5 \\
    1 & 6 \\
  \end{bmatrix} \ \ \text{e} \ \
  \vec{b} =
  \begin{bmatrix}
    2\\3\\4\\2\\3\\4\\3\\5\\4
  \end{bmatrix}
  \end{equation} Assim,
  \begin{equation}
  A^T A =
  \begin{bmatrix}
    9  & 32  \\
    32  & 136
  \end{bmatrix}  \ \ \text{e} \ \
  A^T \vec{b} =
  \begin{bmatrix}
    30\\114
  \end{bmatrix}.
  \end{equation} A solução de mínimos quadrados, que é a solução de $A^T A \vec{x} = A^T\vec{b}$, é dada por
  \begin{equation}
  b = 54/25 = 2.16, \\ a = 33/100 = 0.33.
  \end{equation} A reta procurada é portanto $y = 0.33 x + 2.16$.
  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=0.5\linewidth]{\dir/Semana13/semana13-reta.png}
    \end{center}
  \end{figure}
\end{example}

\subsection*{Exercícios resolvidos}

\construirExeresol

\subsection*{Exercícios}

\construirExer

\section{Regressão não linear}


Podemos também procurar (em contraste com a seção anterior) por funções não lineares que se ajustem a um conjunto de pontos. Por exemplo, dado um certo conjunto de pontos
\begin{equation}
(x_1, y_1), \ (x_2, y_2), \ \dots, (x_k, y_k),
\end{equation} vamos procurar por uma parábola
\begin{equation}
y = a + bx + cx^2
\end{equation} que melhor se ajuste a este conjunto de pontos, no sentido de que a soma dos quadrados dos erros seja a menor possível. Isto corrensponde a procurar pela solução $(a,b, c)$ de mínimos quadrados do seguinte sistema linear
\begin{equation}
\left\{
  \begin{array}{rl}
    a + b x_1 + c x_1^2 &\!\!\!\!\!= y_1  \\
    a + b x_2 + c x_2^2 &\!\!\!\!\!= y_2  \\
    \vdots &  \\
    a + b x_k + c x_k^2 &\!\!\!\!\!= y_k  \\
  \end{array}
\right. \quad \leftrightsquigarrow  \quad
\begin{bmatrix}
  1 & x_1 & x_1^2 \\
  1 & x_2 & x_2^2 \\
  \vdots & \vdots \\
  1 & x_k & x_k^2 \\
\end{bmatrix}
\begin{bmatrix}
  a \\ b \\ c
\end{bmatrix} =
\begin{bmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_k
\end{bmatrix}.
\end{equation} Vejamos como ficaria a parábola que se ajusta aos dados do Exemplo \ref{exp:idade}:

\begin{example}
  Nossos pontos no Exemplo \ref{exp:idade} são
  \begin{equation}
  (20, 590), \ (32, 410), \ (41, 460), \ (49, 380), \ (66, 350).
  \end{equation} Para encontrar a parábola de melhor ajuste como acima, devemos procurar pela solução de mínimos quadrados do sistema
  \begin{equation}
  \begin{bmatrix}
    1 & 20 & 400 \\
    1 & 32 & 1024 \\
    1 & 41 & 1681 \\
    1 & 49 & 2401 \\
    1 & 66 & 4356 \\
  \end{bmatrix}
  \begin{bmatrix}
    a \\ b \\ c
  \end{bmatrix} =
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix} \ \leftrightsquigarrow \ A \vec{x} = \vec{b}.
  \end{equation} Calculamos (com o auxílio de uma calculadora)
  \begin{equation}
  A^T A =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 \\
    20 & 32 & 41 & 49 & 66 \\
    400 & 1024 & 1681 & 2401 & 4356 \\
  \end{bmatrix}
  \begin{bmatrix}
    1 & 20 & 400 \\
    1 & 32 & 1024 \\
    1 & 41 & 1681 \\
    1 & 49 & 2401 \\
    1 & 66 & 4356 \\
  \end{bmatrix} =
  \begin{bmatrix}
    5 & 208 & 9862 \\
    208 & 9862 & 514834 \\
    9862 & 514834 & 28773874 \\
  \end{bmatrix},
  \end{equation}
  \begin{equation}
  A^T A =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 \\
    20 & 32 & 41 & 49 & 66 \\
    400 & 1024 & 1681 & 2401 & 4356 \\
  \end{bmatrix}
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix} =
  \begin{bmatrix}
    2190 \\ 85500 \\ 3866080
  \end{bmatrix}
  \end{equation} e resolvemos (escalonando, por exemplo, com o auxílio de um computador)
  \begin{equation}
  \begin{bmatrix}
    5 & 208 & 9862 & 2190 \\
    208 & 9862 & 514834 & 85500 \\
    9862 & 514834 & 28773874 & 3866080 \\
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0 & 28482529840/35036713 \\
    0 & 1 & 0 & -1505841055/105110139 \\
    0 & 0 & 1 & 11779375/105110139 \\
  \end{bmatrix}.
  \end{equation} Logo, aproximando estas frações, obtemos
  \begin{equation}
  \left\{
    \begin{array}{ll}
      a \simeq 812.934 \\
      b \simeq -14.326 \\
      c \simeq  0.112 \\
    \end{array}
  \right.
  \end{equation} e a parábola desejada é, aproximadamente
  \begin{equation}
  y = 812.934 - 14.326 x + 0.112 x^2.
  \end{equation}
  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=1\linewidth]{\dir/Semana13/semana13-idade-parabola.png}
    \end{center}
  \end{figure}

  \noindent Observamos que, apesar de o erro quadrático ser provavelmente menor, esta parábola se torna crescente a partir de um certo momento, fazendo com que o modelo não seja tão razoável para idades maiores. Por exemplo, é pouco provável que (na média), pessoas com 95 anos vejam a uma distância maior do que pessoas de 65 anos, como sugere a parábola acima.
\end{example}


\subsection*{Exercícios resolvidos}

\construirExeresol

\subsection*{Exercícios}

\construirExer

\section{Regressão linear múltipla}

Uma \textbf{regressão linear múltipla} é o problema análogo à regressão linear simples no caso em que a variável dependente pode depender de mais fatores independentes. Tipicamente, queremos encontrar uma equação afim que melhor se ajusta a alguns dados conhecidos.

No caso especial em que $y$ depende de apenas outros dois fatores, escreveremos
\begin{equation}
z = a + b x + c y,
\end{equation} que, geometricamente, representa um plano no espaço tridimensional $\mathbb{R}^3$. Seja agora uma conjunto de dados:
\begin{equation}
(x_1, y_1, z_1), \ (x_1, y_1, z_1), \dots, (x_k, y_k, z_k).
\end{equation} Queremos encontrar coeficientes $(a,b,c)$ que satisfaçam:
\begin{equation}
\left\{
  \begin{array}{c}
    a + b x_1 + c y_1 = z_1 \\
    a + b x_2 + c y_2 = z_2 \\
    \vdots \\
    a + b x_k + c y_k = z_k \\
  \end{array}
\right. \quad \leftrightsquigarrow \quad
\begin{bmatrix}
  1 & x_1 & y_1 \\
  1 & x_2 & y_2 \\
  \vdots & \vdots & \vdots \\
  1 & x_k & y_k \\
\end{bmatrix}
\begin{bmatrix}
  a \\ b \\ c
\end{bmatrix} =
\begin{bmatrix}
  z_1 \\ z_2 \\ \vdots \\ z_k
\end{bmatrix}.
\end{equation} Quanto maior o número de dados, mais provável de o sistema ser impossível (podemos fazer a analogia geométrica de que por três pontos não colineares no espaço passa um único plano; se aumentarmos o número de pontos, mais difícil que haja um plano contendo todos eles). Por isto, procuramos por uma solução de mínimos quadrados.



\begin{example}
  Uma pesquisa com $214$ mulheres na em uma universidade americana\footnote{Referência: \url{https://onlinecourses.science.psu.edu/stat501/node/292}} coletou informações sobre a altura das participantes, assim como a altura de seus pais. Abaixo, listamos \textit{apenas alguns destes dados}, para que nossas contas não fiquem tão extensas. Fizemos também uma mudança de unidades nas alturas (de polegadas) para centímetros,
  \begin{center}
    \begin{tabular}{|c|c|c|}
      \hline
      % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
      Altura & Altura Mãe & Altura Pai \\ \hline
      152 & 155 & 165 \\
      162 & 155 & 160 \\
      165 & 170 & 173 \\
      170 & 163 & 183 \\
      173 & 168 & 183 \\
      183 & 165 & 183 \\
      \hline
    \end{tabular}
  \end{center} Queremos encontrar uma solução de mínimos quadrados para o sistema linear
  \begin{equation}
  \begin{bmatrix}
    1 & 155 & 165 \\
    1 & 155 & 160 \\
    1 & 170 & 173 \\
    1 & 163 & 183 \\
    1 & 168 & 183 \\
    1 & 165 & 183 \\
  \end{bmatrix}
  \begin{bmatrix}
    a \\ b \\ c
  \end{bmatrix} =
  \begin{bmatrix}
    152  \\
    162  \\
    165  \\
    170  \\
    173  \\
    183  \\
  \end{bmatrix} \ \leftrightsquigarrow \ A \vec{x} = \vec{b}.
  \end{equation} Calculamos
  \begin{equation}
  A^TA =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 & 1 \\
    155 & 155 & 170 & 163 & 168 & 165 \\
    165 & 160 & 173 & 183 & 183 & 183 \\
  \end{bmatrix}
  \begin{bmatrix}
    1 & 155 & 165 \\
    1 & 155 & 160 \\
    1 & 170 & 173 \\
    1 & 163 & 183 \\
    1 & 168 & 183 \\
    1 & 165 & 183 \\
  \end{bmatrix} =
  \begin{bmatrix}
    6 & 976 & 1047 \\
    976 & 158968 & 170553 \\
    1047 & 170553 & 183221 \\
  \end{bmatrix}
  \end{equation} e
  \begin{equation}
  A^T \vec{b} =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 & 1 \\
    155 & 155 & 170 & 163 & 168 & 165 \\
    165 & 160 & 173 & 183 & 183 & 183 \\
  \end{bmatrix}
  \begin{bmatrix}
    152  \\
    162  \\
    165  \\
    170  \\
    173  \\
    183  \\
  \end{bmatrix} =
  \begin{bmatrix}
    1005  \\
    163689  \\
    175803 \\
  \end{bmatrix}.
  \end{equation} Por escalonamento,
  \begin{equation}
  \begin{bmatrix}
    6    & 976    & 1047   & 1005     \\
    976  & 158968 & 170553 & 163689   \\
    1047 & 170553 & 183221 & 175803   \\
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0  & 2154573/145769 \\
    0 & 1 & 0  & 14475/145769   \\
    0 & 0 & 1  & 114081/145769  \\
  \end{bmatrix} \implies
  \left\{
    \begin{array}{ll}
      a \simeq 14.781  \\
      b \simeq  0.099  \\
      c \simeq  0.783  \\
    \end{array}
  \right.
  \end{equation} A equação de melhor ajuste procurada é, portanto, aproximadamente,
  \begin{equation}
  z \simeq 14.781 + 0.099 x + 0.783 y.
  \end{equation} Tente calcular sua altura $z$ a partir da altura de sua mãe $x$ e de seu pai $y$. O teste deve funcionar melhor para mulheres!
  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=1\linewidth]{\dir/Semana13/semana13-alturas.png}
    \end{center}
  \end{figure}
\end{example}


\subsection*{Exercícios resolvidos}

\construirExeresol

\subsection*{Exercícios}

\construirExer

\section{Exercícios finais}

\construirExer

\end{document}
